1 - 5) exp-fullft-0, exp-fullft-1, exp-fullft-2, exp-fullft-3, exp-fullft-4
seed=[42, 1, 2 ,3, 4]
gpu: T4 16gb

training_args = dict(
    output_dir="full_ft_output",
    logging_dir='full_ft_logs',
    logging_strategy="steps",
    evaluation_strategy="steps",

    warmup_steps=200,
    weight_decay=0.01,

    learning_rate=5e-05,
    num_train_epochs=10,
    logging_steps=25,
    eval_steps=25,
    logging_first_step=True,
    per_device_eval_batch_size=16,
    per_device_train_batch_size=16,)

6 - 10) exp-lora-0, exp-lora-1, exp-lora-2, exp-lora-3, exp-lora-4 

seed=[42, 1, 2, 3, 4]
gpu: T4 16gb

lora_config = dict(
    r=8,
    lora_alpha=8,
    target_modules=(["query", "value"]),
    lora_dropout=0.1,
    bias="none",
    modules_to_save=["classifier"],
    task_type="SEQ_CLS"
)

training_args = dict(
    output_dir="lora_output",
    logging_dir='lora_logs',
    logging_strategy="steps",
    evaluation_strategy="steps",
    

    warmup_steps=200,
    weight_decay=0.01,
   
    learning_rate=5e-4,
    num_train_epochs=10,
    logging_steps=25,
    eval_steps=25,
    logging_first_step=True,
    per_device_eval_batch_size=16,
    per_device_train_batch_size=16,)


11, 12) exp-lora-5, exp-lora-6 (low lr)

seed=[42, 1]
gpu: T4 16gb

lora_config = dict(
    r=8,
    lora_alpha=8,
    target_modules=(["query", "value"]),
    lora_dropout=0.1,
    bias="none",
    modules_to_save=["classifier"],
    task_type="SEQ_CLS"
)

training_args = dict(
    output_dir="lora_output",
    logging_dir='lora_logs',
    logging_strategy="steps",
    evaluation_strategy="steps",
    

    warmup_steps=200,
    weight_decay=0.01,
   
    learning_rate=5e-5,
    num_train_epochs=10,
    logging_steps=25,
    eval_steps=25,
    logging_first_step=True,
    per_device_eval_batch_size=16,
    per_device_train_batch_size=16,
    )

13, 14) exp-lora-7, exp-lora-8 (high lr)

seed=[42, 1]
gpu: T4 16gb

lora_config = dict(
    r=8,
    lora_alpha=8,
    target_modules=(["query", "value"]),
    lora_dropout=0.1,
    bias="none",
    modules_to_save=["classifier"],
    task_type="SEQ_CLS"
)

training_args = dict(
    output_dir="lora_output",
    logging_dir='lora_logs',
    logging_strategy="steps",
    evaluation_strategy="steps",
    

    warmup_steps=200,
    weight_decay=0.01,
   
    learning_rate=5e-3,
    num_train_epochs=10,
    logging_steps=25,
    eval_steps=25,
    logging_first_step=True,
    per_device_eval_batch_size=16,
    per_device_train_batch_size=16,
    )

15-19) exp-lora-9-13

seed=[42, 1, 2, 3, 4]
gpu: T4 16gb

lora_config = dict(
    r=8,
    lora_alpha=16,
    target_modules=(["query", "value"]),
    lora_dropout=0.1,
    bias="none",
    modules_to_save=["classifier"],
    task_type="SEQ_CLS"
)

training_args = dict(
    output_dir="lora_output",
    logging_dir='lora_logs',
    logging_strategy="steps",
    evaluation_strategy="steps",
    

    warmup_steps=200,
    weight_decay=0.01,
   
    learning_rate=5e-4,
    num_train_epochs=10,
    logging_steps=25,
    eval_steps=25,
    logging_first_step=True,
    per_device_eval_batch_size=16,
    per_device_train_batch_size=16,
    )

In the folllowing experiments all the other params are the same:

20-24) exp-lora-14-18
seed=[42, 1, 2, 3, 4]
r=16, a=16,

25-29) exp-lora-19-23
seed=[42, 1, 2, 3, 4]
r=16, a=32,

30-34) exp-lora-24-28
seed=[42, 1, 2, 3, 4]
r=4, a=4,

35, 36) exp-lora-29-30
r=4, a=8, seed=42, 1

37, 38 ) exp-lora-31-32
r=4, a=16, seed=42, 1

39, 40) exp-lora-33-34
r=4, a=32, seed=42, 1

41, 42) exp-lora-35-36
r=2, a=2, seed=42, 1

43, 44) exp-lora-37-38
r=2, a=4, seed=42, 1

45, 46) exp-lora-39-40
r=2, a=8, seed=42, 1

47, 48) exp-lora-41-42
r=1, a=1, seed=42, 1

49, 50) exp-lora-43-44
r=1, a=4, seed=42, 1

51, 52) exp-lora-45-46
r=1, a=8, seed=42, 1

53, 54) exp-lora-47-48
r=1, a=16, seed=42, 1

55, 56) exp-lora-49-50
r=8, a=4, seed=42, 1

57, 58) exp-lora-51-52
r=8, a=2, seed=42, 1

59, 60) exp-lora-53-54
r=16, a=8, seed=42, 1

61, 62) exp-lora-55-56
r=16, a=4, seed=42, 1

63, 64) exp-lora-57-58
r=16, a=2, seed=42, 1

65, 66) exp-lora-59-60
r=2, a=16, seed=42, 1

67, 68) exp-lora-61-62
r=4, a=2, seed=42, 1

69 - 73) exp-lora-63-67
r=16, a=64, seed=42, 1, 2, 3, 4

74 - 78) exp-lora-68-72
r=16, a=128, seed=42, 1, 2, 3, 4

79, 80) exp-lora-73-74
r=32, a=16, seed=42, 1

81, 82) exp-lora-75-76
r=32, a=32, seed=42, 1

83, 84) exp-lora-77-78
r=64, a=16, seed=42, 1

85, 86) exp-lora-79-80
r=128, a=16, seed=42, 1

87 - 89) exp-lora-81-83
r=16, a=16, target_modules=(["query", "value", "key"]), seed=42, 1, 2